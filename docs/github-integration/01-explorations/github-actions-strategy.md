<!-- Generated by Graft | model=bedrock-claude-v4.5-sonnet-us | sig=22f6ac9258c633c369bf11f938b498401bbf73f8e3c6d33d18785287e99d9102 | rev=4155622 | 2025-11-04 -->
# GitHub Actions Implementation Strategy

This document defines the implementation strategy for Graft's GitHub Actions workflows, based on its technical architecture and design philosophy.

## 1. Docker Integration

### Image Strategy
**Decision: Pull pre-built images from GitHub Container Registry**

- Build and publish `ghcr.io/owner/graft:latest` on merge to main
- Use `ghcr.io/owner/graft:sha-<commit>` for pinned versions
- PR workflows pull the latest stable image
- Avoid rebuilding on every PR (5-10 minute overhead)

### Workspace Mounting
```yaml
docker run \
  -v $GITHUB_WORKSPACE:/workspace \
  -w /workspace \
  --env-file <(env | grep '^AWS_\|^GITHUB_') \
  ghcr.io/owner/graft:latest \
  graft <command>
```

**Key requirements:**
- Mount entire workspace at `/workspace` (preserves relative paths)
- Set working directory to `/workspace`
- Pass AWS and GitHub credentials via environment
- Use `--user $(id -u):$(id -g)` to maintain file ownership

### Credential Passing
Environment variables only:
- `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` (or)
- `AWS_ROLE_ARN` and `AWS_WEB_IDENTITY_TOKEN_FILE` (OIDC)
- `AWS_REGION` (explicit, don't rely on defaults)
- No volume mounting of `~/.aws/` (security risk)

### Performance Optimization
- Enable Docker layer caching via actions/cache
- Cache key: `docker-graft-${{ runner.os }}-${{ hashFiles('Dockerfile') }}`
- Pull image in separate step before job matrix
- Pre-pull reduces per-job startup by ~30 seconds

## 2. AWS Authentication

### Recommended Approach: OIDC Federation
**Primary method for production use**

```yaml
- uses: aws-actions/configure-aws-credentials@v4
  with:
    role-to-assume: arn:aws:iam::123456789012:role/GitHubActionsGraftRole
    aws-region: us-east-1
```

**Setup requirements:**
1. Create OIDC identity provider in AWS IAM
2. Trust policy: `sts:AssumeRoleWithWebIdentity` for `repo:owner/repo:*`
3. Role permissions: `bedrock:InvokeModel` for Claude models only
4. No long-lived credentials in GitHub

**Advantages:**
- No secret management
- Automatic credential rotation
- Scoped per-repository
- Audit trail via CloudTrail

### Fallback: GitHub Secrets
**For quick setup or AWS accounts without OIDC**

Secrets required:
- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_REGION`

**Security measures:**
- Use dedicated IAM user with minimal permissions
- Restrict to Bedrock InvokeModel only
- Enable CloudTrail logging
- Rotate credentials quarterly
- Consider IP restrictions (GitHub Actions IP ranges)

### Regional Considerations
- Default region: `us-east-1` (most Claude models available)
- Allow override via `AWS_REGION` in workflow dispatch
- Document region-specific model availability
- Handle `ModelNotFoundException` gracefully

## 3. Workflow Triggers

### Primary Trigger: Pull Request Events
```yaml
on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'docs/**/*.md'
      - 'prompts/**/*.md'
      - 'sources/**/*'
      - 'dvc.yaml'
      - 'dvc.lock'
```

**Rationale:**
- Validates docs are current before merge
- Runs on every new commit to PR
- Path filters reduce unnecessary runs

### Secondary Trigger: Push to Main
```yaml
on:
  push:
    branches: [main]
    paths:
      - 'prompts/**/*.md'
      - 'sources/**/*'
```

**Purpose:**
- Regenerate docs if merged without regeneration
- Catch main branch direct pushes
- Auto-fix workflow (optional)

### Manual Trigger: Workflow Dispatch
```yaml
on:
  workflow_dispatch:
    inputs:
      target_file:
        description: 'Specific output file to regenerate (or "all")'
        required: false
        default: 'all'
      force:
        description: 'Force regeneration even if up-to-date'
        type: boolean
        default: false
```

**Use cases:**
- Emergency regeneration
- Testing workflow changes
- Force refresh after model updates

### Optional: Issue Comment Triggers
**Not recommended for initial implementation**

Potential slash commands in PR comments:
- `/graft regenerate` - regenerate all stale docs
- `/graft regenerate docs/config.md` - regenerate specific file

**Complexity concerns:**
- Requires separate workflow listening to `issue_comment` events
- Permission model more complex (who can trigger?)
- Race conditions with concurrent pushes
- Defer to Phase 2

## 4. Job Structure

### Two-Job Architecture

#### Job 1: Validate (Fast Path)
```yaml
validate:
  runs-on: ubuntu-latest
  timeout-minutes: 5
  outputs:
    stale_files: ${{ steps.check.outputs.stale_files }}
    is_stale: ${{ steps.check.outputs.is_stale }}
```

**Responsibilities:**
- Check DVC pipeline status (`dvc status`)
- Identify stale outputs
- Validate dependency graph
- Set outputs for generation job

**Expected runtime:** 30-60 seconds
- Docker pull (cached): 10s
- DVC status check: 5s
- Git operations: 5s

#### Job 2: Generate (Conditional)
```yaml
generate:
  runs-on: ubuntu-latest
  needs: validate
  if: needs.validate.outputs.is_stale == 'true'
  timeout-minutes: 30
```

**Responsibilities:**
- Call `dvc repro` for stale targets
- Commit regenerated outputs
- Push to PR branch

**Expected runtime:** 2-15 minutes per file
- Depends on prompt complexity
- AWS Bedrock API latency: 5-30s
- Multiple files run sequentially (no parallel generation)

### Caching Strategy

#### Docker Image Cache
```yaml
- uses: actions/cache@v3
  with:
    path: /tmp/docker-image
    key: graft-docker-${{ runner.os }}-${{ hashFiles('Dockerfile') }}
    restore-keys: graft-docker-${{ runner.os }}-
```

#### DVC Cache (Future)
Not implemented initially (requires S3 DVC remote setup)
- Would enable reuse of LLM outputs across branches
- Cost savings for identical generations
- Complexity: cache invalidation, storage costs

### Dependency Between Jobs
- `generate` has `needs: validate`
- `generate` only runs if `validate.outputs.is_stale == 'true'`
- Both jobs share Docker image cache
- No parallelization (sequential by design)

## 5. Validation Logic

### Core Validation: DVC Status Check
```bash
dvc status --show-json
```

**Determines staleness by:**
1. **Source changes**: `dvc status` shows modified dependencies
2. **Prompt changes**: Prompt file modified in git
3. **Missing outputs**: Output file doesn't exist in git HEAD
4. **Lock mismatch**: `dvc.lock` out of sync with `dvc.yaml`

### Validation Steps

#### Step 1: DVC Pipeline Integrity
```bash
dvc dag --show-json  # Verify no circular dependencies
dvc status           # Check all outputs vs dependencies
```

**Failures:**
- Circular dependency detected → Exit with error message
- `dvc.yaml` syntax error → Report line number
- Missing source file → List missing dependencies

#### Step 2: Dependency Existence
```bash
for each stage in dvc.yaml:
  for each dep in stage.deps:
    if not exists(dep):
      echo "Missing dependency: $dep for target $stage.outs[0]"
      exit 1
```

**Checks:**
- All source files exist
- All prompt files exist
- No broken references

#### Step 3: Output Freshness
```bash
graft status  # Wrapper around dvc status
```

**Logic:**
- Compare git HEAD versions of deps vs outputs
- Flag as stale if any dep changed since output last regenerated
- Use git commit timestamps as proxy for freshness

#### Step 4: Metadata Verification
Check that output files have valid Graft signature:
```bash
head -n 1 docs/*.md | grep "<!-- Generated by Graft"
```

**Ensures:**
- Outputs were generated by Graft (not manually edited)
- Signature matches expected format
- Revision hash matches git history

### Staleness Determination Algorithm
```python
def is_stale(output_file):
    stage = get_stage_for_output(output_file)
    
    # Check if output exists in git HEAD
    if not git_file_exists(output_file):
        return True
    
    # Get last commit that modified output
    output_commit = git_last_commit(output_file)
    
    # Check all dependencies
    for dep in stage['deps']:
        dep_commit = git_last_commit(dep)
        if dep_commit is newer than output_commit:
            return True
    
    # Check if dvc.lock is stale
    if dvc_status_shows_changes(stage):
        return True
    
    return False
```

## 6. Generation Strategy

### Decision: Auto-Commit to PR Branch
**Chosen approach for best developer experience**

#### Regeneration Flow
1. Validation job detects stale files
2. Generation job runs `dvc repro <target>`
3. Generated files are committed to PR branch
4. Commit is pushed back to PR
5. PR updates automatically with new content

#### Commit Attribution
```yaml
- name: Commit regenerated docs
  run: |
    git config user.name "graft-bot[bot]"
    git config user.email "graft-bot[bot]@users.noreply.github.com"
    git add docs/
    git commit -m "docs: regenerate stale documentation
    
    Auto-generated by Graft CI
    Files updated: ${{ steps.generate.outputs.files }}
    Triggered by: ${{ github.event.pull_request.head.sha }}"
```

**Attribution model:**
- Bot account for commits (clear provenance)
- Commit message includes trigger SHA
- Original author credited in PR, not individual commits

#### Conflict Handling
**Race condition scenario:**
1. PR has commits A → B
2. Workflow starts generation at B
3. Developer pushes commit C while workflow runs
4. Workflow tries to push to branch now at C

**Mitigation:**
```bash
git fetch origin ${{ github.head_ref }}
git rebase origin/${{ github.head_ref }}
if [ $? -ne 0 ]; then
  echo "Rebase conflict detected. Manual intervention required."
  exit 1
fi
git push origin ${{ github.head_ref }}
```

**Fallback:**
- If rebase fails, job fails with clear message
- Developer must pull, regenerate locally, and push
- Rare in practice (most conflicts are in source files, not generated docs)

#### Alternative Considered: Validation-Only Mode
**Not chosen, but available as fallback**

- Job validates only, doesn't auto-commit
- Fails check if docs are stale
- Requires developer to run `dvc repro` locally
- Commit regenerated docs manually

**Rationale for rejection:**
- Adds friction to PR workflow
- Developers forget to regenerate
- Defeats purpose of automation
- Can be enabled via flag if needed

## 7. Performance Optimization

### Docker Image Caching
**Impact: 30-60 second savings per job**

```yaml
- name: Cache Docker image
  uses: actions/cache@v3
  with:
    path: /tmp/graft-image.tar
    key: graft-${{ hashFiles('Dockerfile', 'pyproject.toml') }}

- name: Load cached image
  run: |
    if [ -f /tmp/graft-image.tar ]; then
      docker load < /tmp/graft-image.tar
    else
      docker pull ghcr.io/owner/graft:latest
      docker save ghcr.io/owner/graft:latest > /tmp/graft-image.tar
    fi
```

### Parallel Job Execution
**Not applicable for generation**

- DVC runs stages sequentially (by design)
- Each output depends on specific prompt + sources
- No shared state to parallelize

**Validation can be parallel-aware:**
- Check all stages concurrently
- Report all stale files at once
- Generation still sequential

### Smart Detection of Required Regeneration
**DVC handles this natively**

```bash
dvc repro <target>
```

- Only regenerates if dependencies changed
- Uses checksums, not timestamps
- Skips up-to-date targets automatically

**Additional optimization:**
```bash
# Only run repro for files changed in this PR
CHANGED_SOURCES=$(git diff --name-only origin/main...HEAD | grep '^sources/')
for source in $CHANGED_SOURCES; do
  AFFECTED_TARGETS=$(dvc dag --downstream $source)
  dvc repro $AFFECTED_TARGETS
done
```

### Timeout Handling
**Per-job timeouts:**
- Validate job: 5 minutes (should take <1 minute)
- Generate job: 30 minutes (allows for multiple regenerations)

**Per-file timeout:**
```bash
timeout 10m graft generate <file>
```

**Timeout failure handling:**
- Log which file timed out
- Continue with remaining files (optional)
- Or fail entire job (safer)

**Recommended:** Fail entire job on timeout
- Prevents partially-updated docs
- Forces investigation of slow generations
- Can increase timeout if legitimately needed

### DVC Remote Cache (Future)
**Not implemented in Phase 1**

Potential setup:
```yaml
dvc remote add s3cache s3://graft-dvc-cache/
dvc remote default s3cache
dvc push  # After successful generation
```

**Benefits:**
- Reuse generations across branches
- Faster PR builds (pull from cache)
- Cost savings on redundant API calls

**Complexity:**
- Requires S3 bucket setup
- Cache invalidation strategy
- Storage costs vs. API costs
- Defer until proven necessary

## 8. Error Handling

### Error Categories and Recovery

#### 1. LLM API Failures

**Rate Limits:**
```
Error: ThrottlingException: Rate exceeded for model
```
**Recovery:**
- Exponential backoff (5s, 15s, 45s)
- Retry up to 3 times
- If still failing, report as job failure
- Log request ID for AWS support

**Timeouts:**
```
Error: ReadTimeoutError: Model invocation timed out
```
**Recovery:**
- Single retry with extended timeout
- If retry fails, abort generation
- Report file that failed

**Model Unavailable:**
```
Error: ModelNotFoundException: claude-v4.5-sonnet not available in us-east-1
```
**Recovery:**
- No automatic retry (config issue)
- Clear error message with available models
- Link to AWS Bedrock model availability docs

#### 2. Missing AWS Credentials
```
Error: NoCredentialsError: Unable to locate credentials
```

**Detection:**
```bash
if [ -z "$AWS_ACCESS_KEY_ID" ] && [ -z "$AWS_ROLE_ARN" ]; then
  echo "::error::AWS credentials not configured. See docs/ci-setup.md"
  exit 1
fi
```

**Error message:**
```
AWS credentials not found. 
Configure either:
1. GitHub Secrets: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
2. OIDC: Set up aws-actions/configure-aws-credentials

See: https://github.com/owner/repo/blob/main/docs/ci-setup.md
```

#### 3. Dependency Resolution Failures
```
Error: dvc status failed: Dependency 'sources/config.json' not found
```

**Recovery:**
- No retry (requires fixing source files)
- List all missing dependencies
- Annotate PR with specific files

**Helpful output:**
```
The following source files are missing:
- sources/config.json (required by docs/configuration.md)
- sources/api/endpoints.yaml (required by docs/api-reference.md)

Ad
