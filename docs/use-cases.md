<!-- Generated by Graft | model=bedrock-claude-v4.5-sonnet-us | sig=c1e130cfc737db6fa273e82b942fe9db38c176f8ba59f8b896d2a7ee55d67cfb | rev=9488421 | 2025-11-04 -->
# Use Cases

Graft is an LLM-powered documentation pipeline that combines git-based change detection with multi-level documentation hierarchies. It tracks changes to source files and prompt instructions separately, applying intelligent updates (UPDATE for source changes, RESTYLE for instruction changes) to minimize unnecessary regeneration.

For design-oriented workflows, Graft integrates with Claude Skills through the steward skill, which helps explore documentation architecture and implement solutions iteratively. See [claude-skills.md](claude-skills.md) for detailed examples of working from vague goals to complete information architectures.

This document shows five concrete applications across different domains. The first three demonstrate standard implementations with specific workflows. The last two show the design process—starting from qualitative goals and iterating toward working architectures. These examples are meant to spark ideas you can adapt to your own documentation challenges.

## Core Use Cases

### 1. PR-to-Release Notes Pipeline

**Scenario:** Your team merges dozens of PRs each sprint, each with varying levels of detail in their descriptions. You need release notes that synthesize these changes into clear, user-facing announcements without manually rewriting each PR summary.

**Implementation:**

```yaml
---
deps:
  - releases/sprint-42/pr-summaries.md
  - releases/sprint-42/breaking-changes.md
---

Generate user-facing release notes from PR summaries. Group by feature area (API, UI, Performance, Bug Fixes). Highlight breaking changes prominently. Use plain language—avoid internal jargon and ticket numbers.
```

Create a script that exports merged PRs to `pr-summaries.md` at sprint end. The release notes regenerate automatically when you update summaries or add breaking change details.

**Workflow:**

```bash
# Export PRs merged this sprint
bin/export-prs --sprint 42 > releases/sprint-42/pr-summaries.md

# Add breaking change details manually
vim releases/sprint-42/breaking-changes.md

# Generate release notes
bin/graft rebuild

# Review output
cat releases/sprint-42/release-notes.md
```

**What This Enables:** Release notes stay synchronized with actual changes throughout the sprint. You can iterate on PR summaries (clarifying unclear descriptions) and see the release notes update automatically. When stakeholders ask "what changed?", you have a current, readable answer—not a list of commit hashes.

### 2. Living API Documentation

**Scenario:** Your API evolves rapidly. The OpenAPI spec changes weekly, code examples need updating, and conceptual guides should reflect current capabilities. Manually synchronizing these pieces leads to documentation drift.

**Implementation:**

```yaml
---
deps:
  - api/openapi.yaml
  - api/examples/authentication.py
  - api/examples/pagination.py
---

Generate comprehensive API documentation that combines the OpenAPI spec with working code examples. For each endpoint, show the spec details followed by a practical example. Explain authentication and pagination patterns using the example code.
```

The documentation regenerates when the spec or examples change. Claude applies UPDATE actions to modify only affected sections—if you add a new endpoint, only that section regenerates while existing documentation stays unchanged.

**Workflow:**

```bash
# Update OpenAPI spec
vim api/openapi.yaml

# Update example code
vim api/examples/authentication.py

# Regenerate docs (only changed sections update)
bin/graft rebuild

# Check which sections changed
git diff docs/api/reference.md

# Deploy updated docs
bin/deploy-docs
```

**What This Enables:** API documentation becomes a view of your actual API, not a separate artifact that drifts. Engineers update specs and examples in their normal workflow. Documentation updates automatically. When you ship API changes, the docs are already current.

### 3. Incident Post-Mortem Synthesis

**Scenario:** After a production incident, information lives in multiple places—error logs, timeline notes, updated runbooks, and team retrospective discussions. You need a comprehensive post-mortem that synthesizes these sources into a coherent analysis with clear action items.

**Implementation:**

```yaml
---
deps:
  - incidents/2024-03-15-api-outage/timeline.md
  - incidents/2024-03-15-api-outage/error-logs.txt
  - incidents/2024-03-15-api-outage/runbook-updates.md
  - incidents/2024-03-15-api-outage/team-retro.md
---

Create a comprehensive incident post-mortem with sections: Executive Summary, Timeline, Root Cause Analysis, Impact Assessment, Action Items. Synthesize error logs into technical analysis. Extract action items from runbook updates and retrospective. Keep the executive summary non-technical.
```

**Workflow:**

```bash
# Create incident directory
mkdir -p incidents/2024-03-15-api-outage

# Capture timeline during incident
vim incidents/2024-03-15-api-outage/timeline.md

# Export relevant logs after resolution
bin/export-logs --start "2024-03-15 14:00" > incidents/2024-03-15-api-outage/error-logs.txt

# Document runbook improvements
vim incidents/2024-03-15-api-outage/runbook-updates.md

# Add team retrospective notes
vim incidents/2024-03-15-api-outage/team-retro.md

# Generate comprehensive post-mortem
bin/graft rebuild

# Review and share
cat incidents/2024-03-15-api-outage/post-mortem.md
```

**What This Enables:** Post-mortems capture everything without manual copying and pasting. As you gather more information (updated logs, additional retrospective points), the post-mortem regenerates to incorporate new insights. The synthesis creates a coherent narrative from fragmented sources. Action items from multiple sources get consolidated into a single, prioritized list.

### 4. Strategic Planning Hierarchy (Design Process)

**Starting Point:** "We need better planning documentation. Right now our company strategy, engineering priorities, and team roadmaps live in different places and don't connect. Teams don't understand how their work relates to company goals."

**Initial Exploration:**

You start by identifying what exists and what's missing. You have:
- A company strategy document (updated quarterly by leadership)
- Engineering meeting notes (scattered across wikis)
- Team roadmaps (varying formats, some outdated)

The problem isn't lack of information—it's lack of structure connecting these levels. You need to design an information architecture that creates clear relationships.

**First Attempt:**

You consider putting everything in one document: "Let's create a master planning document that includes strategy, engineering analysis, and all team plans."

This feels wrong. A single document would be too long, and changes to team plans would trigger regeneration of the entire document including strategy sections that didn't change. You need something more granular.

**Revised Approach - Layers:**

You realize planning happens at distinct levels with different audiences:
1. **Company Strategy** (leadership) - quarterly goals, market positioning
2. **Engineering Analysis** (engineering leadership) - how engineering supports strategy
3. **Team Plans** (individual teams) - specific projects and timelines

Each level depends on the one above it. This suggests a hierarchy.

**Exploring the Structure:**

```
docs/00-sources/
  strategy/
    company-strategy-q1-2024.md     # Manual, written by leadership

docs/01-explorations/
  strategy/
    engineering-analysis.prompt.md   # Depends on: company-strategy
                                    # Generates: engineering-analysis.md

docs/02-frameworks/
  strategy/
    backend-team-plan.prompt.md     # Depends on: engineering-analysis.md
    frontend-team-plan.prompt.md    # Depends on: engineering-analysis.md
    infrastructure-team-plan.prompt.md  # Depends on: engineering-analysis.md
```

You test this with one team. The backend team plan prompt:

```yaml
---
deps:
  - docs/01-explorations/strategy/engineering-analysis.md
  - docs/00-sources/teams/backend-capacity.md
---

Create a quarterly plan for the backend team that directly supports the engineering priorities identified in the analysis. For each priority, identify specific projects the backend team will execute. Include timeline estimates based on team capacity. Explain how each project connects to company strategy.
```

**Testing and Iteration:**

You generate the backend team plan. It works, but something's missing—the plan doesn't account for cross-team dependencies. The frontend team needs API endpoints from backend. Infrastructure changes affect everyone.

You add a synthesis layer:

```
docs/03-integration/
  strategy/
    integrated-roadmap.prompt.md    # Depends on: all team plans
                                    # Identifies dependencies, conflicts, sequencing
```

Now when any team plan changes, the integrated roadmap regenerates to show the updated dependency graph. When company strategy changes quarterly, everything cascades: engineering analysis updates, team plans regenerate with new priorities, integrated roadmap reflects the changes.

**What You Learned:**

The final architecture has four levels:
1. **Source** - Company strategy (manual, quarterly)
2. **Exploration** - Engineering analysis (generated from strategy)
3. **Framework** - Individual team plans (generated from analysis + team capacity)
4. **Integration** - Cross-team roadmap (generated from all team plans)

Changes cascade intelligently. Update company strategy → engineering analysis regenerates → team plans update → integrated roadmap reflects new dependencies. But update one team's capacity constraints → only that team's plan and the integrated roadmap regenerate.

The structure emerged from understanding the information flows and dependencies, not from a template. You started with a vague goal, explored different arrangements, discovered what was missing, and adjusted until the architecture matched how your organization actually works.

**Working Implementation:**

```bash
# Quarterly: leadership updates strategy
vim docs/00-sources/strategy/company-strategy-q1-2024.md
bin/graft rebuild
# → engineering-analysis.md regenerates
# → all team plans regenerate
# → integrated-roadmap.md regenerates

# Weekly: team updates capacity or constraints
vim docs/00-sources/teams/backend-capacity.md
bin/graft rebuild
# → only backend-team-plan.md regenerates
# → only integrated-roadmap.md regenerates
# → other team plans unchanged

# Check cascade before major changes
bin/graft uses docs/00-sources/strategy/company-strategy-q1-2024.md
# Shows: engineering-analysis.prompt.md depends on this
bin/graft uses docs/01-explorations/strategy/engineering-analysis.md
# Shows: backend-team-plan.prompt.md, frontend-team-plan.prompt.md, infrastructure-team-plan.prompt.md
```

### 5. Research Synthesis to Product Brief (Design Process)

**Starting Point:** "We just finished user research—20 interviews, survey results, usage analytics, and support tickets. We need to turn this into actionable product decisions, but I don't know how to organize it. Right now it's just a pile of data."

**Understanding the Gap:**

You have raw information but no structure for turning it into decisions. You need to design a process that goes from data → insights → recommendations → specific product changes. But you don't know what the intermediate steps should look like.

**First Attempt - Direct Synthesis:**

Your first thought: "Let's just create a product brief directly from all the research."

You create a prompt:

```yaml
---
deps:
  - research/interviews/all-transcripts.md
  - research/survey-results.csv
  - research/analytics-export.json
  - research/support-tickets.md
---

Create a product brief with recommendations based on all research.
```

You run it. The output is overwhelming—a 30-page document that tries to address everything at once. There's no clear narrative. Recommendations conflict. You can't tell which insights are well-supported vs. speculative.

The problem: you asked Claude to do too much at once. Analysis and synthesis are different cognitive tasks. You need intermediate steps.

**Revised Approach - Separate Analysis:**

What if you analyze each research source separately first?

```
docs/01-explorations/research/
  interview-analysis.prompt.md      # Depends on: interviews
  survey-analysis.prompt.md         # Depends on: survey data
  analytics-analysis.prompt.md      # Depends on: usage data
  support-analysis.prompt.md        # Depends on: support tickets
```

Each analysis prompt has specific instructions:

```yaml
---
deps:
  - research/interviews/all-transcripts.md
---

Analyze interview transcripts. Identify recurring themes, pain points, and feature requests. For each theme, note which users mentioned it and in what context. Rate confidence (high/medium/low) based on how many users mentioned it and how strongly.
```

This is better. Each analysis is focused and manageable. You can review them individually. But now you have four separate analyses—you still need to synthesize them.

**Adding Synthesis Layer:**

```
docs/02-frameworks/research/
  cross-source-synthesis.prompt.md  # Depends on: all four analyses
```

The synthesis prompt:

```yaml
---
deps:
  - docs/01-explorations/research/interview-analysis.md
  - docs/01-explorations/research/survey-analysis.md
  - docs/01-explorations/research/analytics-analysis.md
  - docs/01-explorations/research/support-analysis.md
---

Synthesize findings across all research sources. Identify insights that appear in multiple sources (high confidence) vs. single sources (needs validation). Note contradictions—where different sources suggest different conclusions. Group insights by theme: usability, performance, missing features, workflow issues.
```

You generate this. It's good—it shows which findings are well-supported and which need more investigation. But you realize it's still not actionable. Insights aren't the same as recommendations.

**Adding Recommendations Layer:**

You need another step that translates insights into concrete product changes:

```
docs/03-integration/research/
  product-recommendations.prompt.md  # Depends on: cross-source-synthesis
```

```yaml
---
deps:
  - docs/02-frameworks/research/cross-source-synthesis.md
  - docs/00-sources/product/current-roadmap.md
  - docs/00-sources/product/technical-constraints.md
---

Based on research synthesis, create specific product recommendations. For each recommendation: describe the change, explain which research insights support it, estimate impact (high/medium/low), note technical constraints or dependencies. Identify recommendations that align with current roadmap vs. require roadmap changes. Prioritize based on user impact and implementation difficulty.
```

Now you're getting somewhere. The recommendations are concrete and justified. But product briefs aren't just for the product team—executives and stakeholders need a summary.

**Adding Executive Summary:**

Final layer:

```
docs/04-artifacts/research/
  executive-brief.prompt.md         # Depends on: product-recommendations
```

```yaml
---
deps:
  - docs/03-integration/research/product-recommendations.md
---

Create a 2-page executive brief. First page: key findings and strategic implications—what we learned about user needs and market position. Second page: top 3 recommendations with business justification—why these changes matter for growth/retention/revenue. Use plain language, no jargon. Include specific metrics where available.
```

**The Complete Architecture:**

You now have a five-level information architecture:

```
Level 0 (Sources):
  - research/interviews/
  - research/survey-results.csv
  - research/analytics-export.json
  - research/support-tickets.md
  - product/current-roadmap.md
  - product/technical-constraints.md

Level 1 (Exploration):
  - interview-analysis.md
  - survey-analysis.md
  - analytics-analysis.md
  - support-analysis.md

Level 2 (Framework):
  - cross-source-synthesis.md

Level 3 (Integration):
  - product-recommendations.md

Level 4 (Artifacts):
  - executive-brief.md
```

**What This Enables:**

When you get new survey responses, you update `survey-results.csv`. Graft regenerates:
1. `survey-analysis.md` (UPDATE - new data incorporated)
2. `cross-source-synthesis.md` (UPDATE - new survey insights added)
3. `product-recommendations.md` (UPDATE - recommendations adjusted based on new data)
4. `executive-brief.md` (UPDATE - brief reflects updated recommendations)

When you discover you misunderstood something in interviews, you can update `interview-analysis.prompt.md` with new instructions: "Pay special attention to comments about mobile usage—previous analysis missed this theme." That prompt change triggers RESTYLE for that analysis, then UPDATE for everything downstream.

When the roadmap changes, you update `current-roadmap.md`. Only `product-recommendations.md` and `executive-brief.md` regenerate—the research analyses stay unchanged because the research itself hasn't changed.

**Working Implementation:**

```bash
# Initial setup: create all analyses
bin/graft new interview-analysis research
bin/graft new survey-analysis research
bin/graft new analytics-analysis research
bin/graft new support-analysis research
# Edit each prompt with specific analysis instructions
bin/graft rebuild

# Create synthesis
bin/graft new cross-source-synthesis research
# Edit prompt to compare across sources
bin/
